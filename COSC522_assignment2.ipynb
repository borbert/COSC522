{"cells":[{"cell_type":"markdown","metadata":{"id":"WTjH8z5p5gmx"},"source":["# Assignment #2 Sound Detection"]},{"cell_type":"markdown","metadata":{"id":"eKqs9aR9KGCs"},"source":["Test  and able to edit the notebook! Looks good - Will LaForge"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16443,"status":"ok","timestamp":1666096415458,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"},"user_tz":240},"id":"MGVjisWY5gBK","outputId":"19859e11-317c-4055-9fdf-d4f458988867"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount the local google drive for access to the collected data\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"rf1hotFy1xee","executionInfo":{"status":"ok","timestamp":1666096415459,"user_tz":240,"elapsed":4,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"outputs":[],"source":["# root path for the collected data\n","root_dir='./drive/MyDrive/Colab Notebooks/COSC522/Assignment2/collected_data'"]},{"cell_type":"markdown","metadata":{"id":"RRMP0EwonJXJ"},"source":["## Read in raw data files"]},{"cell_type":"code","execution_count":397,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":212,"status":"ok","timestamp":1666101463042,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"},"user_tz":240},"id":"ZJuHrYvt0h7J","outputId":"3c1b05a2-96ff-4edf-843c-a8abbe3a92fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["44100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]}],"source":["from pandas.core.dtypes.common import classes_and_not_datetimelike\n","'''\n","Find the collected samples by class and store in lists related to each class\n","classes_data is a dictonary of the numpy arrays of data where the class is the key and the arrays are the data\n","'''\n","import os\n","import librosa\n","import scipy.io.wavfile as wavfile\n","import numpy as np\n","\n","classes_data=[]\n","raw_micro_data=[]\n","raw_blender_data=[]\n","raw_music_data=[]\n","raw_siren_data=[]\n","raw_vaccum_data=[]\n","raw_control_data=[]\n","\n","classes=['microwave','blender','music','siren','vaccum_cleaner', 'control']\n","\n","# dict to capture raw data by class\n","classes_data = {\n","    'microwave':raw_micro_data, \n","    'blender':raw_blender_data, \n","    'music':raw_music_data, \n","    'siren':raw_siren_data, \n","    'vaccum_cleaner':raw_vaccum_data, \n","    'control': raw_control_data\n","    }\n","class_dict={\n","    'microwave':1, \n","    'blender':2, \n","    'music':3, \n","    'siren':4, \n","    'vaccum_cleaner':5, \n","    'control': 0\n","}\n","\n","# iterate through classes (also the folder structure where raw data is stored)\n","for cls in classes:\n","  file_path = f\"{root_dir}/{cls}/\"\n","  # iterate through each file in the \"class\" folder\n","  for file in os.listdir(file_path):\n","    #fs,y = wavfile.read(file_path+file)\n","    y,fs=librosa.load(file_path+file,sr=None,dtype=np.float64)\n","    # add this files data to the raw data of this class\n","    classes_data[cls].append(y)\n","\n","# convert lists back to numpy arrays\n","for cls in classes_data:\n","  for data in classes_data:\n","    data=np.array(data)\n"," \n"," \n","  classes_data[cls]=np.array(classes_data[cls]) \n","\n","print(fs)"]},{"cell_type":"code","execution_count":541,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1666109194518,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"},"user_tz":240},"id":"iNAlqIkiBuA2","outputId":"859d25aa-e917-409d-be57-17936cef5541"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'microwave': 5},\n"," {'blender': 5},\n"," {'music': 0},\n"," {'siren': 0},\n"," {'vaccum_cleaner': 5},\n"," {'control': 10}]"]},"metadata":{},"execution_count":541}],"source":["[{x:len(classes_data[x])} for x in classes_data]"]},{"cell_type":"code","execution_count":542,"metadata":{"id":"ZU1qy1hUgJhY","executionInfo":{"status":"ok","timestamp":1666109200777,"user_tz":240,"elapsed":132,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"outputs":[],"source":["#len(classes_data['microwave'][0])"]},{"cell_type":"markdown","metadata":{"id":"je2A5z5YgJ4D"},"source":["# Pre-processing\n","___"]},{"cell_type":"code","execution_count":543,"metadata":{"id":"Hoeso8t_8SJ_","executionInfo":{"status":"ok","timestamp":1666109201269,"user_tz":240,"elapsed":4,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"outputs":[],"source":["'''\n","FFT \n","'''\n","# FFT example from Dr. Sai\n","from scipy import signal\n","\n","def FFT(audio,fs,FFT_SIZE=1024):\n","  f,t,ppx = signal.spectrogram(audio, nperseg=FFT_SIZE, fs=fs) #noverlap=FFT_SIZE/2\n","\n","  return ppx"]},{"cell_type":"code","execution_count":544,"metadata":{"id":"kQfjA_BVgLpg","executionInfo":{"status":"ok","timestamp":1666109202607,"user_tz":240,"elapsed":2,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"outputs":[],"source":["# # FFT. Might need to use this to get a 2-D array\n","# from scipy.fftpack import fft\n","\n","# def fft_method(audio, sampling_rate):\n","#     # variables\n","#     T = 1/sampling_rate\n","#     N = len(audio)\n","#     max_val = 1.0/(2.0*T) # max frequency spectrum of the FFT.\n","#     num_vals = N//2  \n","    \n","#     # calculate fft\n","#     yf_all = fft(audio)\n","    \n","#     xf = np.linspace(0.0, max_val, num_vals)\n","#     yf = 2.0/N * np.abs(yf_all[0:num_vals])\n","    \n","#     return xf, yf"]},{"cell_type":"code","execution_count":545,"metadata":{"id":"Xt_ao6WdiCOS","executionInfo":{"status":"ok","timestamp":1666109202737,"user_tz":240,"elapsed":2,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"outputs":[],"source":["# xf,yf=fft_method(classes_data['microwave'][0], fs)\n","\n","# print(xf,yf)"]},{"cell_type":"markdown","metadata":{"id":"QbeuMGPrgfVJ"},"source":["# Data Analysis Pipeline\n","___"]},{"cell_type":"markdown","metadata":{"id":"-eQ_ydnMglQY"},"source":["## 1. Feature Engineering\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WFqowqHWg-sJ"},"source":["### 1.1  Binning"]},{"cell_type":"code","execution_count":546,"metadata":{"id":"Ad0ezcBYgjnm","executionInfo":{"status":"ok","timestamp":1666109203085,"user_tz":240,"elapsed":1,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"outputs":[],"source":["import cv2\n","\n","# function for binning data \n","def bin_data(ppx,fs,num_time_bins,num_freq_bins):\n","  p = FFT(ppx,fs)\n","  return cv2.resize(p[:,:],(num_time_bins,num_freq_bins))"]},{"cell_type":"markdown","metadata":{"id":"MW3qk0INhDas"},"source":["### 1.2 Extracting domain-specific features"]},{"cell_type":"code","execution_count":547,"metadata":{"id":"CJ0136t02nZS","executionInfo":{"status":"ok","timestamp":1666109203677,"user_tz":240,"elapsed":2,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"outputs":[],"source":["'''\n","Functions for extracting domain-specific features\n","'''\n","import numpy as np\n","\n","def microwave_hum(audio):\n","  # looking for the \"hum\" of the microwave between the 1500 and 3000 ms timestamp \n","  # the microwave hum in testing prodcued data points that had a median of 28.721306\n","  ppx=FFT(audio,fs)\n","\n","  # if np.median(ppx[:40,1500:3000]) >= 60 and np.median(ppx[:40,1500:3000]) <=100:\n","  #   return 1\n","  # else:\n","  #   return 0\n","  return np.median(ppx[:50,1000:2500])\n","\n","def binned_microwave_hum(audio):\n","  # Open CV's resize takes (columns,rows) as the input for desired size\n","  ppx=FFT(audio,fs)\n","\n","  resized_pxx=bin_data(ppx,fs,10,10)\n","\n","  return np.median(resized_pxx[0:3,9:10])\n","\n","def pitches(audio,fs):\n","\n","  pitches, magnitudes = librosa.piptrack(y=audio, sr=fs, n_fft=1024)\n","\n","  return np.mean(pitches)"]},{"cell_type":"markdown","metadata":{"id":"2C-UqTfa-9GB"},"source":["### 1.3 Windowing Data"]},{"cell_type":"code","source":["'''\n","Function to truncate audio files to same exact length\n","--must truncate the audio signal length for scaling and normalizaiton to work\n","--this function trims seconds from the begining of the file\n","@parameters:\n","--audio: raw audio file\n","--fs:  sampling rate\n","--max_len: the maximum in seconds that you wish the file to be <default=30 seconds>\n","returns:\n","--truncated signal to desired length\n","'''\n","\n","def trunc_audio(audio, fs, max_len=30):\n","\n","  sig_len=len(audio)/fs\n","  max_len=max_len\n","\n","  if (sig_len > max_len):\n","    \n","    diff=int(round((sig_len-max_len)*fs,0))\n","      # Truncate the signal to the given length\n","    sig = audio[diff:]\n","      \n","    return sig\n"],"metadata":{"id":"b-KvdIVp2vKe","executionInfo":{"status":"ok","timestamp":1666109204399,"user_tz":240,"elapsed":2,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"execution_count":548,"outputs":[]},{"cell_type":"code","execution_count":549,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1666109204991,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"},"user_tz":240},"id":"u_xRqfoH7Lao"},"outputs":[],"source":["\n","'''\n","Windowing and non-windowing\n","'''\n","from skimage.measure import block_reduce\n","\n","# overall average of the entire recording\n","def no_win_average(audio):\n","  return np.average(audio)\n","\n","# calculate an average of a windowed size of data\n","def windowed_average(audio, window_size=10, fs=fs):\n","  window_size = window_size * fs\n","  \n","  # code for selecting a subset of the entire data set\n","  limited_aud=trunc_audio(audio,fs) \n","  avg_every_n_window = np.add.reduceat(limited_aud, np.arange(0, len(limited_aud), window_size))\n","  \n","  return avg_every_n_window\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"VQMDHvNK6JY3","executionInfo":{"status":"ok","timestamp":1666109205370,"user_tz":240,"elapsed":3,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"execution_count":549,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jg2W2DdiFNqO","executionInfo":{"status":"ok","timestamp":1666109205370,"user_tz":240,"elapsed":2,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"}}},"execution_count":549,"outputs":[]},{"cell_type":"code","execution_count":550,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1666109205370,"user":{"displayName":"Bob Owens","userId":"07362819996799795348"},"user_tz":240},"id":"-HqikJSefOVK"},"outputs":[],"source":["'''\n","Function for extracting features\n","--takes raw audio and extracts different features\n","'''\n","\n","def featurize_input(audio):\n","  stft=np.abs(librosa.stft(audio))\n","\n","  fv=[]\n","  # features go here\n","  fv.append(microwave_hum(audio))\n","  fv.append(binned_microwave_hum(audio))\n","  fv.append(no_win_average(audio))\n","  fv.extend(windowed_average(audio, 5, fs))\n","  # zero crossing rate -- screwed up the normalizaiton, scaling and model fit \n","  #fv.extend(np.mean(librosa.feature.zero_crossing_rate(audio, frame_length=1048)))\n","  # mel spectrogram\n","  fv.extend(np.mean(librosa.feature.melspectrogram(audio, sr=fs).T,axis=0))\n","  # pitches of the audio\n","  #fv.extend(pitches(audio,fs))\n","\n","  fv.extend(np.mean(librosa.feature.chroma_stft(S=stft, sr=fs).T,axis=0))\n","\n","  fv=np.array(fv)\n","\n","  return fv "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmnCBc2ahI4S"},"outputs":[],"source":["\n","features=[]\n","\n","for cls in classes_data:\n","  if len(classes_data[cls])>0:\n","\n","    for x in classes_data[cls]:\n","      features.append([featurize_input(x),class_dict[cls]])\n","        \n","features=np.array(features) \n","\n","#features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kto6OkaDs1ZL"},"outputs":[],"source":["import pandas as pd\n","\n","comb_data=pd.DataFrame(features,columns=['features','labels'])\n","comb_data=comb_data.fillna(0)\n","comb_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDJB5tKzpriH"},"outputs":[],"source":["# extract data and labels from dataframe\n","data=np.array(comb_data['features'].tolist())\n","labels=np.array(comb_data['labels'].tolist())"]},{"cell_type":"code","source":["print(data.shape)\n","print(labels.shape)\n","\n","labels=np.array(labels)\n","data[20]"],"metadata":{"id":"F_YJMTrghpBt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_u6HrbbTgoiy"},"source":["## 2. Feature Normalization / Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqrFK7Iog0Bo"},"outputs":[],"source":["import seaborn as sns\n","\n","for i in range(data.shape[1]):\n","  sns.kdeplot(data[:,i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_gCwj-h11w-"},"outputs":[],"source":["from sklearn.preprocessing import RobustScaler, MinMaxScaler\n","\n","#scaler = RobustScaler()\n","scaler = MinMaxScaler()\n","\n","data = scaler.fit_transform(data)\n","for i in range(data.shape[1]):\n","    sns.kdeplot(data[:,i])"]},{"cell_type":"code","source":[],"metadata":{"id":"zN-W8QyRCFOp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvnibVKCg06e"},"source":["## 3. ML Models for classification\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhKHGZjkg5QG"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from imblearn.over_sampling import SMOTE\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# spliting the data into train and test data sets\n","x_train, xtest, y_train, ytest = train_test_split(data, labels, test_size=0.30, random_state=42)\n","\n","# oversampling the test set here just before training the model\n","#sm = SMOTE(random_state=42)\n","#x_train,y_train=sm.fit_resample(x_train,y_train)\n","#print(f\"Resampled Test Data {len(x_train)}, Labels {len(y_train)}\", end='\\n\\n')\n","\n","xtrain,ytrain=x_train,y_train\n","\n","#training the model\n","clf = SVC(kernel='rbf') \n","# clf=KNeighborsClassifier(n_neighbors=3)\n","\n","clf.fit(xtrain, ytrain)\n","cv_scores = cross_val_score(clf, xtrain, ytrain, cv=3)\n","print(f'Average Cross Validation Score from Training:  {cv_scores.mean()}', sep='\\n', end='\\n')\n","print(f'Average Cross Validation Score STD from Training:  {cv_scores.std()}', sep='\\n', end='\\n\\n\\n')\n","\n","#testing the model\n","ypred = clf.predict(xtest)\n","# cm = confusion_matrix(ytest, ypred) #using a crosstab table to display more info about the predictions\n","cm = pd.crosstab(ytest.ravel(), ypred.ravel(), rownames = ['True'], colnames = ['Predicted'], margins = True)\n","cr = classification_report(ytest, ypred)\n","\n","print('Confusion Matrix:', cm, sep='\\n', end='\\n\\n\\n')\n","print('Test Statistics:', cr, sep='\\n', end='\\n\\n\\n')\n","\n","#This is what we will be grading (>95 expected)\n","print('Testing Accuracy:', accuracy_score(ytest, ypred), end='\\n\\n\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slI4JpSq2g65"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}